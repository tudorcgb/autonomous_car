name: "CarNet"
input: "data"
input_shape { 
    dim: 1 
    dim: 3 
    dim: 48 
    dim: 120 
}

# layer {
#     name: "data"
#     type: "Input"
#     top: "data"
#     input_param { 
#         shape: { dim: 1 dim: 3 dim: 48 dim: 120 }
#     }
#     transform_param {
#         scale: 0.00390625
#     }
# }

# layer {
#     name: "data"
#     type: "input"
#     top: "data"
#     input_shape {
#         dim: 1
#         dim: 3
#         dim: 48
#         dim: 120
#     }
#     transform_param {
#         mirror: true
#         crop_size: 227
#         scale: 0.00390625
#     }
# }

# default_forward_type:  FLOAT16
# default_backward_type: FLOAT16

# layer {
#     name: "train_data"
#     type: "HDF5Data"
#     top: "data"
#     top: "label"
#     include {
#         phase: TRAIN
#     }
#     hdf5_data_param {
#         source: "dat_norm/dataset.txt"
#         batch_size: 256
#         shuffle: true
#         # backend: LMDB
#         # scale: 0.00390625
#     }
# }

# layer {
#     name: "test_data"
#     type: "HDF5Data"
#     top: "data"
#     top: "label"
#     include {
#         phase: TEST
#     }
#     hdf5_data_param {
#         source: "dat_norm/dataset.txt"
#         batch_size: 1
#         # shuffle: true
#         # backend: LMDB
#         # scale: 0.00390625
#     }
# }

# First convolutional layer
layer {
    name: "conv1"
    type: "Convolution"
    param { lr_mult: 1. }  # instead of blobs_lr
    param { lr_mult: 2. }  # instead of blobs_lr
    bottom: "data"
    top: "conv1"
    convolution_param {
        num_output: 16
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "norm1"
    type: "BatchNorm"
    bottom: "conv1"
    top: "norm1"
}
layer {
    name: "relu1"
    type: "ReLU"
    bottom: "norm1"
    # top: "norm1"
    top: "relu1"
    # bottom: "conv1"
    # top: "conv1"
}
layer {
    name: "pool1"
    type: "Pooling"
    # bottom: "norm1"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
        pool: MAX
        kernel_size: 3
        stride: 3
    }
}

# Second convolutional layer
layer {
    name: "conv2"
    type: "Convolution"
    param { lr_mult: 1. }  # instead of blobs_lr
    param { lr_mult: 2. }  # instead of blobs_lr
    bottom: "pool1"
    top: "conv2"
    convolution_param {
        num_output: 24
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "norm2"
    type: "BatchNorm"
    bottom: "conv2"
    top: "norm2"
}
layer {
    name: "relu2"
    type: "ReLU"
    bottom: "norm2"
    # top: "norm2"
    # bottom: "conv2"
    top: "relu2"
}
layer {
    name: "pool2"
    type: "Pooling"
    # bottom: "norm2"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
        pool: MAX
        kernel_size: 2
        stride: 2
    }
}

# Third convolutional layer
layer {
    name: "conv3"
    type: "Convolution"
    param { lr_mult: 1. }  # instead of blobs_lr
    param { lr_mult: 2. }  # instead of blobs_lr
    bottom: "pool2"
    top: "conv3"
    convolution_param {
        num_output: 36
        kernel_size: 2
        pad: 1
        stride: 1
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "norm3"
    type: "BatchNorm"
    bottom: "conv3"
    top: "norm3"
}
layer {
    name: "relu3"
    type: "ReLU"
    bottom: "norm3"
    # top: "norm3"
    # bottom: "conv3"
    top: "relu3"
}
layer {
    name: "pool3"
    type: "Pooling"
    # bottom: "norm3"
    bottom: "relu3"
    top: "pool3"
    pooling_param {
        pool: MAX
        kernel_size: 2
        stride: 2
    }
}

# Forth convolutional layer
layer {
    name: "conv4"
    type: "Convolution"
    param { lr_mult: 1. }  # instead of blobs_lr
    param { lr_mult: 2. }  # instead of blobs_lr
    bottom: "pool3"
    top: "conv4"
    convolution_param {
        num_output: 48
        kernel_size: 2
        pad: 1
        stride: 1
            weight_filler {
        type: "xavier"
        }
        bias_filler {
            type: "constant"
        }
    }
}
layer {
    name: "norm4"
    type: "BatchNorm"
    bottom: "conv4"
    top: "norm4"
}
layer {
    name: "relu4"
    type: "ReLU"
    bottom: "norm4"
    # top: "norm4"
    # bottom: "conv4"
    top: "relu4"
}

layer {
    name: "drop"
    type: "Dropout"
    # bottom: "norm4"
    # top: "norm4"
    bottom: "relu4"
    top: "relu4"
    dropout_param {
        dropout_ratio: 0.4
    }
    include {
        phase: TRAIN
    }
}

# Fifth fully connected layer
layer {
    name: "fc5"
    type: "InnerProduct"
    # bottom: "norm4"
    bottom: "relu4"
    top: "fc5"
    inner_product_param {
        num_output: 1024
    }
}

# Sixth fully connected layer
layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "fc5"
    top: "fc6"
    inner_product_param {
        num_output: 256
        weight_filler {
            type: "xavier"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

# Seventh fully connected layer
layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    inner_product_param {
        num_output: 32
        weight_filler {
            type: "xavier"
            std: 0.01
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

# Eigth fully connected layer
layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "fc7"
    top: "fc8"
    inner_product_param {
        num_output: 1
        weight_filler {
            type: "xavier"
            std: 0.1
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

# layer {
#     name: "loss"
#     # type: "HingeLoss"
#     type: "EuclideanLoss"
#     bottom: "fc8"
#     bottom: "label"
#     top: "loss"
#     include {
#         phase: TRAIN
#     }
# }
